{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Evaluation\n",
    "* 두 개 이상의 LLM 생성물을 서로 비교한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def evaluate_pairwise(runs: list, example) -> dict:\n",
    "    # 점수 저장\n",
    "    scores = {}\n",
    "\n",
    "    for i, run in enumerate(runs):\n",
    "        scores[run.id] = i\n",
    "\n",
    "    # 각 예제에 대한 실행 쌍\n",
    "    answer_a = runs[0].outputs[\"answer\"]\n",
    "    answer_b = runs[1].outputs[\"answer\"]\n",
    "    question = example.inputs[\"question\"]\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    grade_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"You are an LLM judge. Compare the following two answers to a question and determine which one is better.\n",
    "    Better answer is the one that is more detailed and informative.\n",
    "    If the answer is not related to the question, it is not a good answer.\n",
    "\n",
    "    \n",
    "    # Question:\n",
    "    {question}\n",
    "    \n",
    "    #Answer A: \n",
    "    {answer_a}\n",
    "        \n",
    "    #Answer B: \n",
    "    {answer_b}\n",
    "        \n",
    "    Output should be either `A` or `B`. Pick the answer that is better.\n",
    "        \n",
    "    #Preference:\"\"\"\n",
    "    )\n",
    "\n",
    "    answer_grader = grade_prompt | llm | StrOutputParser()\n",
    "\n",
    "    score = answer_grader.invoke(\n",
    "        {\"question\": question, \"answer_a\": answer_a, \"answer_b\": answer_b}\n",
    "    )\n",
    "    \n",
    "    if score == \"A\": # A가 더 답변을 잘했다.\n",
    "        scores[runs[0].id] = 1\n",
    "        scores[runs[1].id] = 0\n",
    "    elif score == \"B\": # B가 더 답변을 잘했다.\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 1\n",
    "    else:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 0\n",
    "        \n",
    "    return {\"key\": \"ranked_preference\", \"score\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag import PDFRAG\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def ask_question_with_llm(llm):\n",
    "    rag = PDFRAG(\"data/snow-white.pdf\", llm)\n",
    "\n",
    "    retriever = rag.create_retriever()\n",
    "\n",
    "    rag_chain = rag.create_chain(retriever)\n",
    "\n",
    "    def ask_question(inputs: dict):\n",
    "        context = retriever.invoke(inputs[\"question\"])\n",
    "        context = \"\\n\".join([doc.page_content for doc in context])\n",
    "        return {\n",
    "            \"question\": inputs[\"question\"],\n",
    "            \"context\": context,\n",
    "            \"answer\": rag_chain.invoke(inputs[\"question\"]),\n",
    "        }\n",
    "\n",
    "    return ask_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 무엇을 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 12, 'total_tokens': 33, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-dc05f8cb-c250-40a1-bbf1-829e19697cf7-0', usage_metadata={'input_tokens': 12, 'output_tokens': 21, 'total_tokens': 33, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "gpt3 = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "gpt3.invoke(\"안녕하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o_chain = ask_question_with_llm(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
    "gpt3_chain = ask_question_with_llm(ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0))\n",
    "# Ollama 사용시\n",
    "# ollama_chain = ask_question_with_llm(ChatOllama(model=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MODEL_COMPARE_EVAUATION-889dcc9e' at:\n",
      "https://smith.langchain.com/o/76515ba2-47a2-4225-a546-4c43a1772406/datasets/0e8fd746-beab-4ad6-bb7f-d582af62e3fd/compare?selectedSessions=9e60e24e-14c7-4f95-8657-8be49ab713aa\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213d9d6d2bc74e5c91b22dfc1a1be0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MODEL_COMPARE_EVAUATION-02b3558a' at:\n",
      "https://smith.langchain.com/o/76515ba2-47a2-4225-a546-4c43a1772406/datasets/0e8fd746-beab-4ad6-bb7f-d582af62e3fd/compare?selectedSessions=fcd5f023-684f-40eb-b2c7-65b539f38607\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80c235477ac4bb1867929d74c2cf0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "cot_qa_evaluator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    config={\"llm\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)}, # 평가자\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"context\"],\n",
    "        \"input\": example.inputs[\"question\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset_name = \"RAG_EVALUATION_DATASET\"\n",
    "\n",
    "experiment_result1 = evaluate(\n",
    "    gpt3_chain,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator],\n",
    "    experiment_prefix=\"MODEL_COMPARE_EVAUATION\",\n",
    "    metadata={\n",
    "        \"variant\": \"GPT-3,5-turbo 평가(cot_qa)\"\n",
    "    }\n",
    ")\n",
    "\n",
    "experiment_result2 = evaluate(\n",
    "    gpt4o_chain,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator],\n",
    "    experiment_prefix=\"MODEL_COMPARE_EVAUATION\",\n",
    "    metadata={\n",
    "        \"variant\": \"GPT-4o-mini 평가(cot_qa)\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
