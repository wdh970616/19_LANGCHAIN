{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM As Judge\n",
    "* LLM 모델을 평가자로 설정하여 모델의 성능을 평가하고 개선할 수 있다.\n",
    "\n",
    "### OFF the shelf Evaluator\n",
    "* LangSmith에서 제공하는 기본 평가자 LLM을 사용하여 모델의 출력을 자동으로 평가할 수 있게 된다.\n",
    "\n",
    "**주요 특징**\n",
    "* 사전 정의된 평가 기준 제공\n",
    "* 일관된 평가 방식 적용\n",
    "* 대규모 출력 평가 자동화 가능\n",
    "\n",
    "**필요 정보**\n",
    "* input : 질문, 보통 데이터셋의 Question이 사용된다.\n",
    "* prection : LLM이 생성한 답변\n",
    "* reference : 정답 답변, Context 등 변칙적으로 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'백설공주는 사과를 먹고 쓰러졌습니다.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rag import PDFRAG\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "rag = PDFRAG(\n",
    "    file_path=\"data/snow-white.pdf\",\n",
    "    llm=ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0),\n",
    ")\n",
    "\n",
    "retriever = rag.create_retriever()\n",
    "\n",
    "chain = rag.create_chain(retriever)\n",
    "\n",
    "chain.invoke(\"백설공주는 어떤 과일을 먹고 쓰러졌나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문에 답변하는 함수\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain.invoke(inputs[\"question\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '백설공주는 사과를 먹고 쓰러졌습니다.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_answer = ask_question({\"question\": \"백설공주는 어떤 과일을 먹고 쓰러졌나요?\"})\n",
    "\n",
    "llm_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator prompt 출력을 위한 함수\n",
    "def print_evaluator_prompt(evaluator):\n",
    "    return evaluator.evaluator.prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-Answer Evaluator\n",
    "* 질문(Question)과 답변(Answer)을 평가합니다.\n",
    "\n",
    "* input : 사용자 입력\n",
    "* prediction : LLM이 생성한 답변\n",
    "* reference : 정답 답변\n",
    "\n",
    "**참고** : Evaluator 프롬프트의 변수에는 query(input), result(prediction), answer(reference)로 정의된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "STUDENT ANSWER: student's answer here\n",
      "TRUE ANSWER: true answer here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "STUDENT ANSWER: \u001b[33;1m\u001b[1;3m{result}\u001b[0m\n",
      "TRUE ANSWER: \u001b[33;1m\u001b[1;3m{answer}\u001b[0m\n",
      "GRADE:\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# qa 평가자 생성\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\")\n",
    "\n",
    "print_evaluator_prompt(qa_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAG_EVALUATION-701e8d0c' at:\n",
      "https://smith.langchain.com/o/76515ba2-47a2-4225-a546-4c43a1772406/datasets/0e8fd746-beab-4ad6-bb7f-d582af62e3fd/compare?selectedSessions=7783bd6b-c115-4f21-9ad1-6e3479cc0e41\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d59df7eaf64de280d6bd3ba66421d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"RAG_EVALUATION_DATASET\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    ask_question, # 평가할 함수\n",
    "    data=dataset_name, # 데이터셋 지정\n",
    "    evaluators=[qa_evaluator], # 평가자 지정\n",
    "    experiment_prefix=\"RAG_EVALUATION\", # 실험 이름 지정\n",
    "    metadata={\n",
    "        \"variant\": \"QA Evaluator를 활용한 평가\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context에 기반한 답변 Evaluator\n",
    "**\"context_qa\"**\n",
    "* LLM 체인의 정확성을 판단하는데에 context를 사용하도록 지시\n",
    "\n",
    "**\"cot_qa\"**\n",
    "* 최종 판결을 하기전에 LLM의 추론을 사용하도록 지시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context를 반환하는 RAG 결과 반환 함수\n",
    "def rag_context_answer(inputs: dict):\n",
    "    context = retriever.invoke(inputs[\"question\"])\n",
    "    return {\n",
    "        \"context\": \"\\n\".join([doc.page_content for doc in context]),\n",
    "        \"answer\": chain.invoke,\n",
    "        \"query\":inputs[\"question\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': '왕비는먹음직스럽게생긴사과를골라독을발랐어요.\\n그리고과일장수로변장했지요.\\n왕비는산을넘고또넘어일곱난쟁이의오두막에도착했어요.\\n“새콤달콤맛있는사과가있어요. 아가씨의붉은입술처럼새빨\\n간사과랍니다. 잠깐문을열어보세요.”\\n백설공주는고개를저었어요.\\n“난쟁이들이문을열어주지말라고했어요.”\\n백설공주가거절하자, 왕비는창문틈새로사과를쑥내밀었어\\n요.\\n“그럼, 맛이라도봐요. 정말맛있으니까. 둘이먹다하나가죽어\\n도모를걸요.”\\n“탐스러운사과네. 맛있어보여. 한입만아삭깨물어볼까?”\\n사과를베어문순간, 백설공주는온몸에독이퍼져정신을잃고\\n쓰러졌어요.\\n사과를베어문순간, 백설공주는온몸에독이퍼져정신을잃고\\n쓰러졌어요.\\n“호호호. 이제내가세상에서가장아름답겠지?”\\n왕비는백설공주를버려둔채자리를떠났어요.\\n백설공주\\n옛날어느왕국에공주님이태어났어요.\\n“어쩜이렇게어여쁠까? 살결이눈처럼하얗구나. 백\\n설공주라고불러야겠다.”\\n왕과왕비는갓태어난딸을보며기뻐했어요.\\n하지만기쁨도잠시, 왕비는곧세상을떠나고말았어\\n요.\\n숲속을헤매던백설공주는외딴오두막에이르렀어요.\\n들여다보니오두막은비어있었어요.\\n“아무도없네. 좀쉬어가도될까? 어? 신기하다! 모든게작아. \\n어어? 이상하다! 모든게일곱. 의자도일곱, 접시도일곱. 어머, \\n침대도일곱개네.”\\n도망치느라치진백설공주는식탁위에있던빵을먹고나서\\n일곱번째침대에쓰러져잠들었어요.\\n밤이되자오두막주인인일곱난쟁이가돌아왔어요.\\n난쟁이들은집안이어질러진것을보고깜짝놀랐지요.\\n일곱째난쟁이가큰소리로외쳤어요.\\n“누가내침대에서자고있어!”\\n북적이는소리에잠이깬백설공주는왕비를피해도망쳤다고\\n이야기했어요.',\n",
       " 'answer': <bound method RunnableSequence.invoke of {\n",
       "   context: VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D68E893050>, search_kwargs={}),\n",
       "   question: RunnablePassthrough()\n",
       " }\n",
       " | PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. \\n        Use the following pieces of retrieved context to answer the question. \\n        If you don't know the answer, just say that you don't know. \\n\\n        #Context: \\n        {context}\\n\\n        #Question:\\n        {question}\\n\\n        #Answer:\")\n",
       " | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D68E3A0390>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D68E218790>, root_client=<openai.OpenAI object at 0x000001D68E15F150>, root_async_client=<openai.AsyncOpenAI object at 0x000001D68E3A0550>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       " | StrOutputParser()>,\n",
       " 'query': '백설공주는 어떤 과일을 먹고 쓰러졌나요?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_context_answer(\n",
    "    {\"question\": \"백설공주는 어떤 과일을 먹고 쓰러졌나요?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cot_qa 평가자\n",
    "cot_qa_evaluator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    prepare_data=lambda run, example : {\n",
    "        \"prediction\": run.outputs[\"answer\"], # LLM이 생성한 답변\n",
    "        \"reference\": run.outputs[\"context\"], # Context\n",
    "        \"input\": example.inputs[\"question\"] # 데이터셋의 질문\n",
    "    }\n",
    ")\n",
    "\n",
    "# context_qa 평가자\n",
    "context_qa_evaluator = LangChainStringEvaluator(\n",
    "    \"context_qa\",\n",
    "    prepare_data=lambda run, example : {\n",
    "        \"prediction\": run.outputs[\"answer\"], # LLM이 생성한 답변\n",
    "        \"reference\": run.outputs[\"context\"], # Context\n",
    "        \"input\": example.inputs[\"question\"] # 데이터셋의 질문\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, based on the context.\n",
      "Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "CONTEXT: context the question is about here\n",
      "STUDENT ANSWER: student's answer here\n",
      "EXPLANATION: step by step reasoning here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "CONTEXT: \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "STUDENT ANSWER: \u001b[33;1m\u001b[1;3m{result}\u001b[0m\n",
      "EXPLANATION:\n"
     ]
    }
   ],
   "source": [
    "print_evaluator_prompt(cot_qa_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, based on the context.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "CONTEXT: context the question is about here\n",
      "STUDENT ANSWER: student's answer here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "CONTEXT: \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "STUDENT ANSWER: \u001b[33;1m\u001b[1;3m{result}\u001b[0m\n",
      "GRADE:\n"
     ]
    }
   ],
   "source": [
    "print_evaluator_prompt(context_qa_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAG_EVALUATION-3232c219' at:\n",
      "https://smith.langchain.com/o/76515ba2-47a2-4225-a546-4c43a1772406/datasets/0e8fd746-beab-4ad6-bb7f-d582af62e3fd/compare?selectedSessions=fcb3962c-3637-4267-a91f-07bfeb219b5c\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b647a2b386d4307a98d9b2ce9be32a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults RAG_EVALUATION-3232c219>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 이름\n",
    "dataset_name = \"RAG_EVALUATION_DATASET\"\n",
    "\n",
    "# 평가실행\n",
    "evaluate(\n",
    "    rag_context_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator, context_qa_evaluator],\n",
    "    experiment_prefix=\"RAG_EVALUATION\",\n",
    "    metadata={\n",
    "        \"variant\": \"COT_QA & CONTEXT_QA Evalution을 활용한 평가\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteria\n",
    "* 기준값이 없거나 얻기 힘든 경우 \"criteria\"를 통해 사용자 지정 기준 집합에 대한 실행을 평가할 수 있다.\n",
    "* 답변에 대해 높은 수준의 의미론적 측면을 평가하고자 할 때 유용하다.\n",
    "LangChainStringEvaluator(\"criteria\", config={ \"criteria\": `아래 중 하나의 criterion` })\n",
    "\n",
    "| 기준 | 설명 |\n",
    "|------|------|\n",
    "| `conciseness` | 답변이 간결하고 간단한지 평가 |\n",
    "| `relevance` | 답변이 질문과 관련이 있는지 평가 |\n",
    "| `correctness` | 답변이 옳은지 평가 |\n",
    "| `coherence` | 답변이 일관성이 있는지 평가 |\n",
    "| `harmfulness` | 답변이 해롭거나 유해한지 평가 |\n",
    "| `maliciousness` | 답변이 악의적이거나 악화시키는지 평가 |\n",
    "| `helpfulness` | 답변이 도움이 되는지 평가 |\n",
    "| `controversiality` | 답변이 논란이 되는지 평가 |\n",
    "| `misogyny` | 답변이 여성을 비하하는지 평가 |\n",
    "| `criminality` | 답변이 범죄를 촉진하는지 평가 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'CRITERIA_EVALUATION-e58b002d' at:\n",
      "https://smith.langchain.com/o/76515ba2-47a2-4225-a546-4c43a1772406/datasets/0e8fd746-beab-4ad6-bb7f-d582af62e3fd/compare?selectedSessions=b0e559a9-1ad5-4e5f-8955-24a5de5ede77\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28da85ad71a4c89960cc508867980e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# 평가자 설정\n",
    "criteria_evaluator = [\n",
    "    LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"conciseness\"}),\n",
    "    LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"relevance\"}),\n",
    "    LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"coherence\"})\n",
    "]\n",
    "\n",
    "# 데이터셋 이름 설정]\n",
    "dataset_name = \"RAG_EVALUATION_DATASET\"\n",
    "\n",
    "# 평가 실행\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=criteria_evaluator,\n",
    "    experiment_prefix=\"CRITERIA_EVALUATION\",\n",
    "    metadata={\n",
    "        \"variant\": \"criteria를 활용한 평가\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
