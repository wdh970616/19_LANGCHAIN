{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_template() ë©”ì†Œë“œ\n",
    "* ë³€ìˆ˜ë¥¼ ì¤‘ê´„í˜¸ë¡œ ë¬¶ì–´ì„œ í…œí”Œë¦¿ì— ì •ì˜í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['name'] input_types={} partial_variables={} template='{name}ì˜ ì§ì—…ì€ ë¬´ì—‡ì¸ê°€ìš”?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# í…œí”Œë¦¿ ì •ì˜, {}ì•ˆì˜ ë‚´ìš©ì€ ì´í›„ì— ê°’ì´ ë“¤ì–´ê°ˆ ìë¦¬\n",
    "template = \"{name}ì˜ ì§ì—…ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "\n",
    "# from_template ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ PromptTemplate ê°ì²´ë¥¼ ìƒì„±\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bearì˜ ì§ì—…ì€ ë¬´ì—‡ì¸ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "# prompt ìƒì„±(ì™„ì„±), format() ë©”ì†Œë“œì˜ name ì¸ìë¥¼ ì´ìš©í•˜ì—¬ ë³€ìˆ˜ì— ê°’ì„ ë„£ìŒ\n",
    "prompt = prompt.format(name=\"bear\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language'] input_types={} partial_variables={} template='{language}ëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"{language}ëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pythonì€ ê·€ë„ ë°˜ ë¡œì¸(Guido van Rossum)ì— ì˜í•´ 1989ë…„ì— ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” 1991ë…„ì— Pythonì˜ ì²« ë²ˆì§¸ ë²„ì „ì„ ê³µê°œí•˜ì˜€ê³ , ì´í›„ Pythonì€ ì „ ì„¸ê³„ì ìœ¼ë¡œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ ì„±ì¥í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# print(chain.invoke({\"language\": \"Python\"}).content)\n",
    "# ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€ìˆ˜ë¥¼ ì‘ì„±í•˜ëŠ”ê²Œ ì›ì¹™ì´ì§€ë§Œ ë³€ìˆ˜ê°€ í•˜ë‚˜ì¼ë•ŒëŠ” ë³€ìˆ˜ë§Œìœ¼ë¡œ ì‘ì„± ê°€ëŠ¥\n",
    "print(chain.invoke({\"Python\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplates ê°ì²´ ìƒì„±ê³¼ ë™ì‹œì— prompt ìƒì„±\n",
    "* input_variables ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€ìˆ˜ë¥¼ ì§€ì •í•œë‹¤.\n",
    "* í…œí”Œë¦¿ì— ì‘ì„±í•œ ë³€ìˆ˜ê°€ input_variablesì— ì—†ìœ¼ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language'] input_types={} partial_variables={} template='{language}ëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PythonëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template ì •ì˜\n",
    "template = \"{language}ëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"language\"]\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "prompt.format(language=\"Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partial_variables**\n",
    "* ì—°ì‚°ì¤‘ ë¯¸ë¦¬ ê³„ì‚°ëœ ë³€ìˆ˜ë¥¼ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì§€ì •í•´ì„œ ë„£ì„ ìˆ˜ ìˆë‹¤.\n",
    "* í•­ìƒ ê³µí†µëœ ë°©ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¤ê³  ì‹¶ì€ ë³€ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‚¬ìš©<br>ex)ë‚ ì§œ, ì‹œê°„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language1'] input_types={} partial_variables={'language2': 'Java'} template='{language1}ê³¼ {language2}ëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'\n"
     ]
    }
   ],
   "source": [
    "# template ì •ì˜\n",
    "template = \"{language1}ê³¼ {language2}ëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?\"\n",
    "\n",
    "# PromptTemplate ê°ì²´ ìƒì„±\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"language1\"],\n",
    "    partial_variables={\n",
    "        \"language2\": \"Java\" # dictionary í˜•íƒœë¡œ partial_value ì‘ì„±\n",
    "    }\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pythonê³¼ JavaëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(language1=\"Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partial()**\n",
    "* ì—°ì‚°ì¤‘ì— ë¯¸ë¦¬ ê³„ì‚°ëœ ë³€ìˆ˜ë¥¼ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì§€ì •í•´ì„œ ë„£ì„ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language1'] input_types={} partial_variables={'language2': 'JavaScript'} template='{language1}ê³¼ {language2}ëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'\n"
     ]
    }
   ],
   "source": [
    "prompt_partial = prompt.partial(language2=\"JavaScript\")\n",
    "\n",
    "print(prompt_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='pythonê³¼ JavaScriptëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_partial.invoke(\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pythonì€ ê·€ë„ ë°˜ ë¡œì¸(Guido van Rossum)ì— ì˜í•´ 1991ë…„ì— ì²˜ìŒ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” ì´ ì–¸ì–´ë¥¼ ë§Œë“¤ ë•Œ ì½”ë“œì˜ ê°€ë…ì„±ê³¼ ì‚¬ìš©ì˜ ìš©ì´ì„±ì„ ì¤‘ì‹œí–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "JavaScriptëŠ” ë¸Œë Œë˜ ì•„ì´í¬(Brendan Eich)ì— ì˜í•´ 1995ë…„ì— ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” ë„·ìŠ¤ì¼€ì´í”„(Netscape)ì—ì„œ ì¼í•  ë‹¹ì‹œ ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ë™ì ì¸ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ê¸° ìœ„í•´ ì´ ì–¸ì–´ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. JavaScriptëŠ” ì›¹ ê°œë°œì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê²Œ ë˜ì—ˆê³ , í˜„ì¬ëŠ” ë‹¤ì–‘í•œ í”Œë«í¼ì—ì„œë„ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_partial | llm\n",
    "\n",
    "print(chain.invoke(\"Python\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŒŒì¼ë¡œë¶€í„° template ì½ì–´ì˜¤ê¸°\n",
    "* promptë¥¼ í¸í•˜ê²Œ ì‘ì„± ë° ìˆ˜ì •\n",
    "* ìƒí™©ì— ë§ê²Œ íŒŒì¼ì„ ì‘ì„±í•´ë‘ë©´ í•„ìš”í• ë•Œë§ˆë‹¤ êº¼ë‚´ì„œ ì“¸ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language'] input_types={} partial_variables={} template='{language} ì–¸ì–´ì— ëŒ€í•´ 3ì¤„ë¡œ ì„¤ëª…í•´ì¤˜.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"prompts/language_simple.yml\", encoding=\"UTF-8\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python ì–¸ì–´ì— ëŒ€í•´ 3ì¤„ë¡œ ì„¤ëª…í•´ì¤˜.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(language=\"Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ì–¸ì–´ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜.\n",
      "ê·¸ë¦¬ê³  ì„¤ëª…ì„ ë‹¤ìŒì˜ ì–‘ì‹ì— ë§ê²Œ ì •ë¦¬í•´ì¤˜.\n",
      "300ì ë‚´ì™¸.\n",
      "í•œê¸€ë¡œ ì‘ì„±.\n",
      "---\n",
      "#ì–‘ì‹\n",
      "1. íŠ¹ì§•\n",
      "2. ì œì‘ì\n",
      "3. ëŒ€í‘œì ì¸ í”„ë ˆì„ì›Œí¬\n",
      "4. ë§ì´ ì‚¬ìš©ë˜ëŠ” ë¶„ì•¼\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt2 = load_prompt(\"prompts/language.yml\", encoding=\"UTF-8\")\n",
    "\n",
    "print(prompt2.format(language=\"Python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. íŠ¹ì§•: ìë°”ëŠ” ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, í”Œë«í¼ ë…ë¦½ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. \"í•œ ë²ˆ ì‘ì„±í•˜ë©´ ì–´ë””ì„œë‚˜ ì‹¤í–‰ëœë‹¤\"ëŠ” ìŠ¬ë¡œê±´ ì•„ë˜, JVM(Java Virtual Machine)ì„ í†µí•´ ë‹¤ì–‘í•œ ìš´ì˜ì²´ì œì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë˜í•œ, ê°•ë ¥í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ì™€ ì˜ˆì™¸ ì²˜ë¦¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "2. ì œì‘ì: ìë°”ëŠ” 1995ë…„ ì¬ ë§ˆì´í¬ë¡œì‹œìŠ¤í…œì¦ˆ(Sun Microsystems)ì—ì„œ ì œì„ìŠ¤ ê³ ìŠ¬ë§(James Gosling)ì— ì˜í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. ëŒ€í‘œì ì¸ í”„ë ˆì„ì›Œí¬: ìŠ¤í”„ë§(Spring), í•˜ì´ë²„ë„¤ì´íŠ¸(Hibernate), ìë°” ì„œë²„ í˜ì´ìŠ¤ìŠ¤(JavaServer Faces, JSF) ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. ë§ì´ ì‚¬ìš©ë˜ëŠ” ë¶„ì•¼: ìë°”ëŠ” ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜, ëª¨ë°”ì¼ ì• í”Œë¦¬ì¼€ì´ì…˜(ì•ˆë“œë¡œì´ë“œ), ê¸°ì—…ìš© ì†Œí”„íŠ¸ì›¨ì–´, ë¹…ë°ì´í„° ì²˜ë¦¬ ë° í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt2 | ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0) | StrOutputParser()\n",
    "\n",
    "answer = chain.invoke(\"Java\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "* ëŒ€í™” ëª©ë¡ì„ í”„ë¡¬í”„íŠ¸ë¡œ ì£¼ì…í•˜ê³ ì í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n",
    "* ë©”ì„¸ì§€ëŠ” íŠœí”Œ í˜•íƒœë¡œ ì „ë‹¬\n",
    "    *(\"role\", \"message\")ë¡œ êµ¬ì„±ë˜ê³  ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ìƒì„± ê°€ëŠ¥\n",
    "\n",
    "**role**\n",
    "* system : ì‹œìŠ¤í…œ ì„¤ì • ë©”ì„¸ì§€ë¡œ ì£¼ë¡œ ì „ì—­ ì„¤ì •ì„ í•  ë•Œ ì‚¬ìš©\n",
    "* human : ì‚¬ìš©ìì˜ ì…ë ¥ ë©”ì„¸ì§€\n",
    "* ai : AIì˜ ë‹µë³€ ë©”ì„¸ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='{language}ì˜ ì œì‘ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\"{language}ì˜ ì œì‘ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?\")\n",
    "\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Pythonì˜ ì œì‘ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "print(chat_prompt.format(language=\"Python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì¸ê³µì§€ëŠ¥ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì€ êµ¬ì°Œì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ë°˜ê°€ì›Œìš”!', additional_kwargs={}, response_metadata={}), AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={}), HumanMessage(content='ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    # role, message\n",
    "    (\"system\", \"ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì¸ê³µì§€ëŠ¥ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì€ {name}ì…ë‹ˆë‹¤.\"),\n",
    "    (\"human\", \"ë°˜ê°€ì›Œìš”!\"),\n",
    "    (\"ai\", \"ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    name=\"êµ¬ì°Œ\", user_input=\"ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?\"\n",
    ")\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì œ ì´ë¦„ì€ êµ¬ì°Œì…ë‹ˆë‹¤! ë‹¹ì‹ ê³¼ ëŒ€í™”í•˜ê²Œ ë˜ì–´ ê¸°ì©ë‹ˆë‹¤. ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆŒê¹Œìš”?\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MessagePlaceHolder\n",
    "* ì•„ì§ í™•ì •ëœ ë©”ì„¸ì§€ê°€ ì•„ë‹ˆì§€ë§Œ ë‚˜ì¤‘ì— ì±„ì›Œì§ˆ ë©”ì„¸ì§€ ìœ„ì¹˜ë¥¼ ì¡ì•„ë‘ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤.\n",
    "* ë³´í†µ ëŒ€í™” ê¸°ë¡ì„ í•˜ê³ ì‹¶ì„ ë•Œì— ì‚¬ìš©í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['conversation', 'word_count'] input_types={'conversation': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000002571A2F4A40>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ìš”ì•½ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì—…ë¬´ëŠ” ì£¼ìš” í‚¤ì›Œë“œë¡œ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.'), additional_kwargs={}), MessagesPlaceholder(variable_name='conversation'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['word_count'], input_types={}, partial_variables={}, template='ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ {word_count} ë‹¨ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"ë‹¹ì‹ ì€ ìš”ì•½ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì—…ë¬´ëŠ” ì£¼ìš” í‚¤ì›Œë“œë¡œ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\"),\n",
    "    MessagesPlaceholder(variable_name=\"conversation\"),\n",
    "    (\"human\", \"ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ {word_count} ë‹¨ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.\")\n",
    "])\n",
    "\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: ë‹¹ì‹ ì€ ìš”ì•½ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì—…ë¬´ëŠ” ì£¼ìš” í‚¤ì›Œë“œë¡œ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "Human: íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ ìˆ«ìë¥¼ ì œê±°í•˜ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•˜ë©´ ë ê¹Œ?\n",
      "AI: ì•„ì£¼ ê°„ë‹¨í•´ìš”! set()ì„ ì‚¬ìš©í•˜ë©´ ì¤‘ë³µì„ ì‰½ê²Œ ì œê±°í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤.\n",
      "Human: set()ì´ ë­”ì§€ëŠ” ì˜ ëª¨ë¥´ê² ì§€ë§Œ ì—„ì²­ ê°„ë‹¨í•˜ë„¤! ì´ê±¸ë¡œ í•´ê²°ëì–´ ã…ã…\n",
      "AI: ë„¤, setì€ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ìë£Œí˜•ì´ì—ìš”! ì •ë§ ìœ ìš©í•˜ì£ . ì•ìœ¼ë¡œë„ íŒŒì´ì¬ì˜ í¸ë¦¬í•œ ê¸°ëŠ¥ë“¤ì„ ë§ì´ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆì–´ìš”~ ğŸ˜Š\n",
      "Human: ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ 5 ë‹¨ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "formatted_chat_prompt = chat_prompt.format(\n",
    "    word_count=5,\n",
    "    conversation=[\n",
    "        (\"human\", \"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ ìˆ«ìë¥¼ ì œê±°í•˜ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•˜ë©´ ë ê¹Œ?\"),\n",
    "        (\"ai\", \"ì•„ì£¼ ê°„ë‹¨í•´ìš”! set()ì„ ì‚¬ìš©í•˜ë©´ ì¤‘ë³µì„ ì‰½ê²Œ ì œê±°í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤.\"),\n",
    "        (\"human\", \"set()ì´ ë­”ì§€ëŠ” ì˜ ëª¨ë¥´ê² ì§€ë§Œ ì—„ì²­ ê°„ë‹¨í•˜ë„¤! ì´ê±¸ë¡œ í•´ê²°ëì–´ ã…ã…\"),\n",
    "        (\"ai\", \"ë„¤, setì€ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ìë£Œí˜•ì´ì—ìš”! ì •ë§ ìœ ìš©í•˜ì£ . ì•ìœ¼ë¡œë„ íŒŒì´ì¬ì˜ í¸ë¦¬í•œ ê¸°ëŠ¥ë“¤ì„ ë§ì´ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆì–´ìš”~ ğŸ˜Š\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(formatted_chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì´ì¬, ì¤‘ë³µ ì œê±°, set ì‚¬ìš©, ê°„ë‹¨ í•´ê²°, ìœ ìš©  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"word_count\": 5,\n",
    "    \"conversation\": [\n",
    "        (\"human\", \"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ ìˆ«ìë¥¼ ì œê±°í•˜ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•˜ë©´ ë ê¹Œ?\"),\n",
    "        (\"ai\", \"ì•„ì£¼ ê°„ë‹¨í•´ìš”! set()ì„ ì‚¬ìš©í•˜ë©´ ì¤‘ë³µì„ ì‰½ê²Œ ì œê±°í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤.\"),\n",
    "        (\"human\", \"set()ì´ ë­”ì§€ëŠ” ì˜ ëª¨ë¥´ê² ì§€ë§Œ ì—„ì²­ ê°„ë‹¨í•˜ë„¤! ì´ê±¸ë¡œ í•´ê²°ëì–´ ã…ã…\"),\n",
    "        (\"ai\", \"ë„¤, setì€ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ìë£Œí˜•ì´ì—ìš”! ì •ë§ ìœ ìš©í•˜ì£ . ì•ìœ¼ë¡œë„ íŒŒì´ì¬ì˜ í¸ë¦¬í•œ ê¸°ëŠ¥ë“¤ì„ ë§ì´ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆì–´ìš”~ ğŸ˜Š\")\n",
    "    ]    \n",
    "}\n",
    "\n",
    "print(chain.invoke(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-community) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-community) (3.10.10)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.4 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-community) (0.3.4)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-community) (0.3.13)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-community) (0.1.137)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.16.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.23.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain<0.4.0,>=0.3.4->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain<0.4.0,>=0.3.4->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain-community) (2.23.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\20116\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
      "Downloading langchain_community-0.3.3-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 34.2 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.23.0-py3-none-any.whl (49 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain-community-0.3.3 marshmallow-3.23.0 mypy-extensions-1.0.0 pydantic-settings-2.6.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ” ì•½ 24ì‹œê°„ì…ë‹ˆë‹¤. ì´ë¥¼ **í•˜ë£¨**ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì§€êµ¬ì˜ ìì „ì€ ì™„ë²½í•˜ì§€ ì•Šê³ , ì•½ê°„ì”© ë³€í•©ë‹ˆë‹¤. ë˜í•œ, ì§€êµ¬ê°€ íƒœì–‘ ì£¼ìœ„ë¥¼ ë„ëŠ” íšŒì „(ê³µì „)ë„ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ë•Œë¬¸ì— ëª‡ ê°€ì§€ ê¸°ì¤€ì— ë”°ë¼ ë‹¤ë¥¸ ì‹œê°„ì´ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:\n",
      "\n",
      "* **ì‹œì°¨**: ì§€êµ¬ ìì „ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ì¼ê´‘ì€ ê° ì§€ì—­ì—ì„œ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚˜ë¯€ë¡œ, ì‹œê°ì  ê´€ì ì—ì„œ í•˜ë£¨ì˜ ê¸¸ì´ëŠ” ì§€ì—­ì— ë”°ë¼ ì•½ê°„ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "* **ì¤‘ë ¥ê³¼ í–‰ì„± ê°„ ì˜í–¥**: íƒœì–‘ê³„ì˜ ë‹¤ë¥¸ ì²œì²´ë“¤ë¡œë¶€í„° ì˜¤ëŠ” ì¤‘ë ¥ê³¼ ì „ìê¸°íŒŒê°€ ì§€êµ¬ ìì „ ì£¼ê¸°ì— ë¯¸ì„¸í•œ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì¦‰, ìš°ë¦¬ê°€ ì¼ìƒì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” \"í•˜ë£¨\"ë¼ëŠ” ê°œë…ì€ ì‹¤ì œë¡œ ì§€êµ¬ ìì „ ì£¼ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê·¼ì‚¬ì ì¸ ê°’ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# model\n",
    "llm = Ollama(model=\"gemma2:9b\")\n",
    "\n",
    "response = llm.invoke(\"ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ”?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20116\\AppData\\Local\\Temp\\ipykernel_13784\\2642841664.py:3: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"gemma2:9b\",\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì´ì¬, ì¤‘ë³µ ì œê±°, set ì‚¬ìš©, ê°„ë‹¨ í•´ê²°, ìœ ìš©  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain = chat_prompt | model | StrOutputParser()\n",
    "\n",
    "print(chain.invoke(input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
